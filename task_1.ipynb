{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f492f8-1b86-4d34-9222-accf5a406cf8",
   "metadata": {},
   "source": [
    "## Task 1. Artificial neural network implementation using a classification of choice of any image dataset \n",
    "In this project, we implement the AlexNet architecture for the classification of grapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89d1c7de-4972-4ce2-a332-90f8da11c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import subprocess as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import  layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d511d9f3-15ce-41f5-8d18-b9c447a1f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727248b3-9690-42ce-90ec-e7f4d44cfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(numClasses, inputShape = (100,100,3)):\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size=(11,11), strides=(4,4), padding='valid', activation='relu', input_shape=(100,100,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    model.add(Conv2D(filters=192, kernel_size=(5,5), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dense(NumClasses, activation='softmax')) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976de6de-49df-4666-aad4-add89210386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"C:\\\\Users\\\\Tomi\\\\Desktop\\\\archive\\\\fruits-360_dataset\\\\fruits-360\\\\Training\"\n",
    "test_path = \"C:\\\\Users\\\\Tomi\\\\Desktop\\\\archive\\\\fruits-360_dataset\\\\fruits-360\\\\Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2911b8ed-0e3c-4741-bd0d-455a83d14b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67692 images belonging to 131 classes.\n",
      "Found 11335 images belonging to 131 classes.\n",
      "Found 11353 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define image size and batch size\n",
    "IMAGE_SIZE = (100, 100) \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define paths to your dataset\n",
    "TRAIN_DATA_PATH = train_path\n",
    "VALIDATION_DATA_PATH = test_path\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_PATH,\n",
    "    # shuffle=True,\n",
    "    subset=\"validation\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "test_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_PATH,\n",
    "    # shuffle=True,\n",
    "    subset=\"training\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "NumClasses = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4d8ef2-104a-4776-9ee1-0314e3edd8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of images in dataset: 90380 \n",
      "Number of data samples for Training Dataset: 67692 i.e. 0.75 of total dataset\n",
      "Number of data samples for Validation  Dataset: 11335 i.e. 0.13 of total dataset\n",
      "Number of fata samples for Testing Dataset: 11353 i.e. 0.13 of total dataset\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Number of images in dataset: {train_generator.samples+ validation_generator.samples+ test_generator.samples} ')\n",
    "print(f'Number of data samples for Training Dataset: {train_generator.samples} i.e. {train_generator.samples / (train_generator.samples+ validation_generator.samples+ test_generator.samples):.2f} of total dataset')\n",
    "print(f'Number of data samples for Validation  Dataset: {validation_generator.samples} i.e. {validation_generator.samples / (train_generator.samples+ validation_generator.samples+ test_generator.samples):.2f} of total dataset')\n",
    "print(f'Number of fata samples for Testing Dataset: {test_generator.samples} i.e. {test_generator.samples / (train_generator.samples+ validation_generator.samples+ test_generator.samples):.2f} of total dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b701dc-27ce-4a4f-840f-b7722c9aaa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model = get_model(NumClasses,(100,100,3))\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#This is creating files with weights, so in case if something happens no need to wait again for model to train, can just use created weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_weights_epoch_Alexnet{epoch:02d}.weights.h5', save_weights_only=True, save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36adb08e-e1b2-4ede-abeb-feca8a13c1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 221ms/step - accuracy: 0.9821 - loss: 0.0535 - val_accuracy: 0.9694 - val_loss: 0.0916\n",
      "Epoch 2/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 210ms/step - accuracy: 0.9862 - loss: 0.0412 - val_accuracy: 0.9809 - val_loss: 0.0655\n",
      "Epoch 3/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 210ms/step - accuracy: 0.9837 - loss: 0.0495 - val_accuracy: 0.9869 - val_loss: 0.0599\n",
      "Epoch 4/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 212ms/step - accuracy: 0.9848 - loss: 0.0458 - val_accuracy: 0.9871 - val_loss: 0.0408\n",
      "Epoch 5/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 210ms/step - accuracy: 0.9876 - loss: 0.0380 - val_accuracy: 0.9824 - val_loss: 0.0544\n",
      "Epoch 6/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 214ms/step - accuracy: 0.9884 - loss: 0.0345 - val_accuracy: 0.9837 - val_loss: 0.0504\n",
      "Epoch 7/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 212ms/step - accuracy: 0.9885 - loss: 0.0368 - val_accuracy: 0.9683 - val_loss: 0.0952\n",
      "Epoch 8/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 212ms/step - accuracy: 0.9883 - loss: 0.0360 - val_accuracy: 0.9914 - val_loss: 0.0292\n",
      "Epoch 9/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 214ms/step - accuracy: 0.9912 - loss: 0.0284 - val_accuracy: 0.9780 - val_loss: 0.0989\n",
      "Epoch 10/10\n",
      "\u001b[1m2116/2116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 212ms/step - accuracy: 0.9908 - loss: 0.0292 - val_accuracy: 0.9903 - val_loss: 0.0344\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,validation_data = validation_generator,epochs=EPOCHS,callbacks=[checkpoint_callback,MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d89d8af2-26c0-494b-b123-8ce239fb3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights_epoch_Alexnet10.weights.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feb54705-b058-4b65-a81a-75c439a66f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9669 - loss: 0.1621\n",
      "evaluate compile_metrics: 96.47%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate AlexNet on Validation Data\n",
    "scores = model.evaluate(validation_generator)\n",
    "print(\"%s%s: %.2f%%\" % (\"evaluate \",model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "854a1af6-cb91-430f-8818-9648416f55ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.9709 - loss: 0.1480\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2536aa-e08b-4e78-9f5d-acf837cfef3e",
   "metadata": {},
   "source": [
    "Results as percentage: ~97%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
